{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Kunal\\\\Job_Applications\\\\DeeperSystem\\\\train.rotfaces\\\\train'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"F:\\Kunal\\Job_Applications\\DeeperSystem\\train.rotfaces\\train\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-10049200_1891-09-16_1958.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-10110600_1985-09-17_2012.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-10126400_1964-07-07_2010.jpg</td>\n",
       "      <td>upright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0-1013900_1917-10-15_1960.jpg</td>\n",
       "      <td>rotated_right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0-10166400_1960-03-12_2008.jpg</td>\n",
       "      <td>upside_down</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               fn          label\n",
       "0  0-10049200_1891-09-16_1958.jpg   rotated_left\n",
       "1  0-10110600_1985-09-17_2012.jpg   rotated_left\n",
       "2  0-10126400_1964-07-07_2010.jpg        upright\n",
       "3   0-1013900_1917-10-15_1960.jpg  rotated_right\n",
       "4  0-10166400_1960-03-12_2008.jpg    upside_down"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(r\"F:\\Kunal\\Job_Applications\\DeeperSystem\\train.rotfaces\\train.truth.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0-10049200_1891-09-16_1958.jpg', '0-10110600_1985-09-17_2012.jpg',\n",
       "       '0-10126400_1964-07-07_2010.jpg', '0-1013900_1917-10-15_1960.jpg',\n",
       "       '0-10166400_1960-03-12_2008.jpg'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fn=df['fn'].values\n",
    "train_fn[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-10049200_1891-09-16_1958.jpg',\n",
       " '0-10110600_1985-09-17_2012.jpg',\n",
       " '0-10126400_1964-07-07_2010.jpg',\n",
       " '0-1013900_1917-10-15_1960.jpg',\n",
       " '0-10166400_1960-03-12_2008.jpg']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs=os.listdir()\n",
    "train_imgs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that, file name sequence is not matching in train_imgs and train_fn.\n",
    "\n",
    "Therefore we may wrongly assign label for each file in train_imgs if we take label from df as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48896\n",
      "48896\n"
     ]
    }
   ],
   "source": [
    "print(len(train_fn))\n",
    "print(len(train_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sum(train_fn==train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only 1084 file names are matching with their positions (index) in train_imgs and train_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct={}\n",
    "\n",
    "for index,row in df.iterrows():\n",
    "    dct[row['fn']]=row['label']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48896"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=[dct[f] for f in train_imgs]\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have proper label (label) for each file name (train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the image\n",
    "im=cv2.imread(train_imgs[101],0) \n",
    "im2=cv2.cvtColor(im,cv2.COLOR_GRAY2BGR)\n",
    "cv2.imshow(\"image\",im2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:03<00:00, 1852.83it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reading all image as gray scale image and converting all of them to 3 channel image\n",
    "# And finally storing all the image vectors in matrix(list/array)\n",
    "matrix=[]\n",
    "for img in tqdm(train_imgs[:7000]):\n",
    "    im=cv2.cvtColor(cv2.imread(img,0),cv2.COLOR_GRAY2BGR)\n",
    "    matrix.append(np.array(im))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 64, 64, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting list(matrix) into numpy array\n",
    "matrix=np.array(matrix)\n",
    "matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rotated_left', 'rotated_right', 'upright', 'upside_down'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Playing around with label \n",
    "\n",
    "label=label[:7000]\n",
    "set(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 4 class labels. Lets represent them in numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2, 1, 3, 0, 0, 0, 3, 2]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# representing label in numerical form\n",
    "dct={'rotated_left':0,'rotated_right':1,'upright':2,'upside_down':3}\n",
    "label=[dct[item] for item in label]\n",
    "label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.49411765, 0.49411765, 0.49411765],\n",
       "        [0.47843137, 0.47843137, 0.47843137],\n",
       "        [0.54509804, 0.54509804, 0.54509804],\n",
       "        ...,\n",
       "        [0.50980392, 0.50980392, 0.50980392],\n",
       "        [0.4745098 , 0.4745098 , 0.4745098 ],\n",
       "        [0.48235294, 0.48235294, 0.48235294]],\n",
       "\n",
       "       [[0.55686275, 0.55686275, 0.55686275],\n",
       "        [0.65098039, 0.65098039, 0.65098039],\n",
       "        [0.53333333, 0.53333333, 0.53333333],\n",
       "        ...,\n",
       "        [0.6       , 0.6       , 0.6       ],\n",
       "        [0.48235294, 0.48235294, 0.48235294],\n",
       "        [0.63137255, 0.63137255, 0.63137255]],\n",
       "\n",
       "       [[0.55686275, 0.55686275, 0.55686275],\n",
       "        [0.58431373, 0.58431373, 0.58431373],\n",
       "        [0.43921569, 0.43921569, 0.43921569],\n",
       "        ...,\n",
       "        [0.42745098, 0.42745098, 0.42745098],\n",
       "        [0.4745098 , 0.4745098 , 0.4745098 ],\n",
       "        [0.45490196, 0.45490196, 0.45490196]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.86666667, 0.86666667, 0.86666667],\n",
       "        [0.84313725, 0.84313725, 0.84313725],\n",
       "        [0.88235294, 0.88235294, 0.88235294],\n",
       "        ...,\n",
       "        [0.49411765, 0.49411765, 0.49411765],\n",
       "        [0.58431373, 0.58431373, 0.58431373],\n",
       "        [0.85098039, 0.85098039, 0.85098039]],\n",
       "\n",
       "       [[0.81568627, 0.81568627, 0.81568627],\n",
       "        [0.8627451 , 0.8627451 , 0.8627451 ],\n",
       "        [0.82352941, 0.82352941, 0.82352941],\n",
       "        ...,\n",
       "        [0.5372549 , 0.5372549 , 0.5372549 ],\n",
       "        [0.49803922, 0.49803922, 0.49803922],\n",
       "        [0.76470588, 0.76470588, 0.76470588]],\n",
       "\n",
       "       [[0.83529412, 0.83529412, 0.83529412],\n",
       "        [0.88235294, 0.88235294, 0.88235294],\n",
       "        [0.81176471, 0.81176471, 0.81176471],\n",
       "        ...,\n",
       "        [0.51372549, 0.51372549, 0.51372549],\n",
       "        [0.54117647, 0.54117647, 0.54117647],\n",
       "        [0.58431373, 0.58431373, 0.58431373]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting data type to float and normalizing the values by dividing each value by max value i.e. 255\n",
    "\n",
    "matrix=matrix.astype('float')\n",
    "matrix/=255\n",
    "matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# representing label as one hot encoded vector\n",
    "label=np_utils.to_categorical(label)\n",
    "label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking image\n",
    "im1=matrix[0]\n",
    "cv2.imshow(\"img\",im1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network (model) compilation\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a\\Anaconda_new\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 5600 samples, validate on 1400 samples\n",
      "Epoch 1/10\n",
      "5600/5600 [==============================] - ETA: 3:14 - loss: 1.3963 - accuracy: 0.25 - ETA: 2:15 - loss: 1.4050 - accuracy: 0.25 - ETA: 1:51 - loss: 1.4128 - accuracy: 0.24 - ETA: 1:37 - loss: 1.4054 - accuracy: 0.25 - ETA: 1:27 - loss: 1.4003 - accuracy: 0.25 - ETA: 1:19 - loss: 1.3944 - accuracy: 0.27 - ETA: 1:12 - loss: 1.3892 - accuracy: 0.28 - ETA: 1:06 - loss: 1.3832 - accuracy: 0.29 - ETA: 1:01 - loss: 1.3755 - accuracy: 0.30 - ETA: 55s - loss: 1.3685 - accuracy: 0.3121 - ETA: 50s - loss: 1.3582 - accuracy: 0.323 - ETA: 45s - loss: 1.3494 - accuracy: 0.333 - ETA: 40s - loss: 1.3370 - accuracy: 0.345 - ETA: 35s - loss: 1.3220 - accuracy: 0.361 - ETA: 30s - loss: 1.3040 - accuracy: 0.376 - ETA: 26s - loss: 1.2838 - accuracy: 0.395 - ETA: 21s - loss: 1.2594 - accuracy: 0.412 - ETA: 17s - loss: 1.2345 - accuracy: 0.429 - ETA: 12s - loss: 1.2115 - accuracy: 0.443 - ETA: 8s - loss: 1.1923 - accuracy: 0.454 - ETA: 3s - loss: 1.1736 - accuracy: 0.46 - 105s 19ms/step - loss: 1.1544 - accuracy: 0.4784 - val_loss: 0.6448 - val_accuracy: 0.7679\n",
      "Epoch 2/10\n",
      "5600/5600 [==============================] - ETA: 1:27 - loss: 0.6257 - accuracy: 0.80 - ETA: 1:24 - loss: 0.6281 - accuracy: 0.78 - ETA: 1:20 - loss: 0.5941 - accuracy: 0.79 - ETA: 1:18 - loss: 0.5869 - accuracy: 0.79 - ETA: 1:15 - loss: 0.6074 - accuracy: 0.79 - ETA: 1:11 - loss: 0.6107 - accuracy: 0.79 - ETA: 1:06 - loss: 0.5926 - accuracy: 0.79 - ETA: 1:01 - loss: 0.5835 - accuracy: 0.80 - ETA: 58s - loss: 0.5839 - accuracy: 0.8008 - ETA: 53s - loss: 0.5732 - accuracy: 0.802 - ETA: 49s - loss: 0.5716 - accuracy: 0.804 - ETA: 44s - loss: 0.5772 - accuracy: 0.802 - ETA: 40s - loss: 0.5704 - accuracy: 0.803 - ETA: 35s - loss: 0.5611 - accuracy: 0.808 - ETA: 31s - loss: 0.5554 - accuracy: 0.810 - ETA: 26s - loss: 0.5501 - accuracy: 0.810 - ETA: 22s - loss: 0.5467 - accuracy: 0.810 - ETA: 17s - loss: 0.5456 - accuracy: 0.810 - ETA: 13s - loss: 0.5412 - accuracy: 0.810 - ETA: 8s - loss: 0.5355 - accuracy: 0.812 - ETA: 4s - loss: 0.5326 - accuracy: 0.81 - 108s 19ms/step - loss: 0.5274 - accuracy: 0.8145 - val_loss: 0.4155 - val_accuracy: 0.8436\n",
      "Epoch 3/10\n",
      "5600/5600 [==============================] - ETA: 1:36 - loss: 0.3659 - accuracy: 0.85 - ETA: 1:34 - loss: 0.3684 - accuracy: 0.85 - ETA: 1:29 - loss: 0.3659 - accuracy: 0.86 - ETA: 1:23 - loss: 0.3495 - accuracy: 0.86 - ETA: 1:18 - loss: 0.3465 - accuracy: 0.87 - ETA: 1:14 - loss: 0.3434 - accuracy: 0.86 - ETA: 1:09 - loss: 0.3400 - accuracy: 0.87 - ETA: 1:04 - loss: 0.3468 - accuracy: 0.87 - ETA: 59s - loss: 0.3409 - accuracy: 0.8741 - ETA: 54s - loss: 0.3399 - accuracy: 0.874 - ETA: 50s - loss: 0.3406 - accuracy: 0.874 - ETA: 45s - loss: 0.3411 - accuracy: 0.874 - ETA: 41s - loss: 0.3353 - accuracy: 0.877 - ETA: 36s - loss: 0.3361 - accuracy: 0.876 - ETA: 32s - loss: 0.3356 - accuracy: 0.876 - ETA: 27s - loss: 0.3397 - accuracy: 0.874 - ETA: 22s - loss: 0.3395 - accuracy: 0.875 - ETA: 18s - loss: 0.3383 - accuracy: 0.876 - ETA: 13s - loss: 0.3361 - accuracy: 0.877 - ETA: 8s - loss: 0.3357 - accuracy: 0.877 - ETA: 4s - loss: 0.3326 - accuracy: 0.87 - 109s 20ms/step - loss: 0.3316 - accuracy: 0.8782 - val_loss: 0.3275 - val_accuracy: 0.8786\n",
      "Epoch 4/10\n",
      "5600/5600 [==============================] - ETA: 1:32 - loss: 0.1729 - accuracy: 0.95 - ETA: 1:30 - loss: 0.2286 - accuracy: 0.92 - ETA: 1:25 - loss: 0.2342 - accuracy: 0.92 - ETA: 1:20 - loss: 0.2310 - accuracy: 0.91 - ETA: 1:16 - loss: 0.2338 - accuracy: 0.91 - ETA: 1:11 - loss: 0.2363 - accuracy: 0.91 - ETA: 1:07 - loss: 0.2408 - accuracy: 0.90 - ETA: 1:03 - loss: 0.2391 - accuracy: 0.90 - ETA: 58s - loss: 0.2374 - accuracy: 0.9084 - ETA: 53s - loss: 0.2363 - accuracy: 0.908 - ETA: 49s - loss: 0.2402 - accuracy: 0.908 - ETA: 44s - loss: 0.2381 - accuracy: 0.909 - ETA: 40s - loss: 0.2355 - accuracy: 0.911 - ETA: 36s - loss: 0.2383 - accuracy: 0.911 - ETA: 31s - loss: 0.2362 - accuracy: 0.912 - ETA: 27s - loss: 0.2366 - accuracy: 0.914 - ETA: 22s - loss: 0.2362 - accuracy: 0.915 - ETA: 17s - loss: 0.2304 - accuracy: 0.917 - ETA: 13s - loss: 0.2265 - accuracy: 0.919 - ETA: 8s - loss: 0.2246 - accuracy: 0.919 - ETA: 4s - loss: 0.2227 - accuracy: 0.92 - 108s 19ms/step - loss: 0.2248 - accuracy: 0.9191 - val_loss: 0.2769 - val_accuracy: 0.9029\n",
      "Epoch 5/10\n",
      "5600/5600 [==============================] - ETA: 1:39 - loss: 0.1392 - accuracy: 0.96 - ETA: 1:36 - loss: 0.1274 - accuracy: 0.95 - ETA: 1:30 - loss: 0.1429 - accuracy: 0.94 - ETA: 1:24 - loss: 0.1522 - accuracy: 0.94 - ETA: 1:19 - loss: 0.1461 - accuracy: 0.95 - ETA: 1:13 - loss: 0.1515 - accuracy: 0.95 - ETA: 1:08 - loss: 0.1498 - accuracy: 0.95 - ETA: 1:03 - loss: 0.1551 - accuracy: 0.95 - ETA: 59s - loss: 0.1548 - accuracy: 0.9501 - ETA: 54s - loss: 0.1541 - accuracy: 0.949 - ETA: 49s - loss: 0.1493 - accuracy: 0.951 - ETA: 44s - loss: 0.1518 - accuracy: 0.948 - ETA: 40s - loss: 0.1499 - accuracy: 0.948 - ETA: 35s - loss: 0.1481 - accuracy: 0.948 - ETA: 31s - loss: 0.1458 - accuracy: 0.949 - ETA: 26s - loss: 0.1434 - accuracy: 0.950 - ETA: 21s - loss: 0.1439 - accuracy: 0.950 - ETA: 17s - loss: 0.1458 - accuracy: 0.950 - ETA: 13s - loss: 0.1435 - accuracy: 0.951 - ETA: 8s - loss: 0.1436 - accuracy: 0.950 - ETA: 3s - loss: 0.1458 - accuracy: 0.95 - 108s 19ms/step - loss: 0.1442 - accuracy: 0.9505 - val_loss: 0.2459 - val_accuracy: 0.9264\n",
      "Epoch 6/10\n",
      "5600/5600 [==============================] - ETA: 1:33 - loss: 0.1186 - accuracy: 0.96 - ETA: 1:28 - loss: 0.0899 - accuracy: 0.97 - ETA: 1:24 - loss: 0.0900 - accuracy: 0.97 - ETA: 1:19 - loss: 0.0948 - accuracy: 0.97 - ETA: 1:17 - loss: 0.0952 - accuracy: 0.97 - ETA: 1:13 - loss: 0.0946 - accuracy: 0.97 - ETA: 1:08 - loss: 0.0939 - accuracy: 0.97 - ETA: 1:04 - loss: 0.0990 - accuracy: 0.96 - ETA: 59s - loss: 0.0970 - accuracy: 0.9688 - ETA: 54s - loss: 0.1036 - accuracy: 0.967 - ETA: 50s - loss: 0.1049 - accuracy: 0.966 - ETA: 45s - loss: 0.1072 - accuracy: 0.965 - ETA: 40s - loss: 0.1093 - accuracy: 0.964 - ETA: 35s - loss: 0.1094 - accuracy: 0.964 - ETA: 31s - loss: 0.1095 - accuracy: 0.964 - ETA: 26s - loss: 0.1088 - accuracy: 0.964 - ETA: 22s - loss: 0.1071 - accuracy: 0.965 - ETA: 17s - loss: 0.1082 - accuracy: 0.964 - ETA: 12s - loss: 0.1059 - accuracy: 0.965 - ETA: 8s - loss: 0.1061 - accuracy: 0.964 - ETA: 3s - loss: 0.1064 - accuracy: 0.96 - 106s 19ms/step - loss: 0.1046 - accuracy: 0.9648 - val_loss: 0.2398 - val_accuracy: 0.9314\n",
      "Epoch 7/10\n",
      "5600/5600 [==============================] - ETA: 1:32 - loss: 0.0701 - accuracy: 0.97 - ETA: 1:28 - loss: 0.0650 - accuracy: 0.97 - ETA: 1:23 - loss: 0.0610 - accuracy: 0.97 - ETA: 1:19 - loss: 0.0544 - accuracy: 0.98 - ETA: 1:14 - loss: 0.0527 - accuracy: 0.98 - ETA: 1:10 - loss: 0.0511 - accuracy: 0.98 - ETA: 1:07 - loss: 0.0537 - accuracy: 0.98 - ETA: 1:02 - loss: 0.0591 - accuracy: 0.98 - ETA: 58s - loss: 0.0611 - accuracy: 0.9809 - ETA: 54s - loss: 0.0609 - accuracy: 0.980 - ETA: 50s - loss: 0.0604 - accuracy: 0.980 - ETA: 46s - loss: 0.0593 - accuracy: 0.980 - ETA: 41s - loss: 0.0622 - accuracy: 0.979 - ETA: 37s - loss: 0.0624 - accuracy: 0.979 - ETA: 32s - loss: 0.0608 - accuracy: 0.979 - ETA: 27s - loss: 0.0627 - accuracy: 0.979 - ETA: 22s - loss: 0.0660 - accuracy: 0.978 - ETA: 18s - loss: 0.0676 - accuracy: 0.977 - ETA: 13s - loss: 0.0677 - accuracy: 0.977 - ETA: 8s - loss: 0.0675 - accuracy: 0.977 - ETA: 4s - loss: 0.0687 - accuracy: 0.97 - 110s 20ms/step - loss: 0.0691 - accuracy: 0.9762 - val_loss: 0.2686 - val_accuracy: 0.9250\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5600/5600 [==============================] - ETA: 1:44 - loss: 0.0526 - accuracy: 0.97 - ETA: 1:37 - loss: 0.0507 - accuracy: 0.98 - ETA: 1:30 - loss: 0.0496 - accuracy: 0.97 - ETA: 1:24 - loss: 0.0505 - accuracy: 0.97 - ETA: 1:18 - loss: 0.0512 - accuracy: 0.97 - ETA: 1:13 - loss: 0.0471 - accuracy: 0.98 - ETA: 1:08 - loss: 0.0505 - accuracy: 0.97 - ETA: 1:03 - loss: 0.0509 - accuracy: 0.98 - ETA: 58s - loss: 0.0509 - accuracy: 0.9805 - ETA: 53s - loss: 0.0524 - accuracy: 0.980 - ETA: 49s - loss: 0.0524 - accuracy: 0.980 - ETA: 44s - loss: 0.0547 - accuracy: 0.980 - ETA: 39s - loss: 0.0532 - accuracy: 0.981 - ETA: 35s - loss: 0.0542 - accuracy: 0.980 - ETA: 30s - loss: 0.0554 - accuracy: 0.980 - ETA: 26s - loss: 0.0548 - accuracy: 0.980 - ETA: 21s - loss: 0.0550 - accuracy: 0.979 - ETA: 17s - loss: 0.0551 - accuracy: 0.979 - ETA: 12s - loss: 0.0545 - accuracy: 0.979 - ETA: 8s - loss: 0.0544 - accuracy: 0.979 - ETA: 3s - loss: 0.0580 - accuracy: 0.97 - 106s 19ms/step - loss: 0.0583 - accuracy: 0.9793 - val_loss: 0.3043 - val_accuracy: 0.9264\n",
      "Epoch 9/10\n",
      "5600/5600 [==============================] - ETA: 1:32 - loss: 0.0411 - accuracy: 0.98 - ETA: 1:28 - loss: 0.0451 - accuracy: 0.99 - ETA: 1:24 - loss: 0.0645 - accuracy: 0.98 - ETA: 1:20 - loss: 0.0520 - accuracy: 0.98 - ETA: 1:15 - loss: 0.0526 - accuracy: 0.98 - ETA: 1:10 - loss: 0.0520 - accuracy: 0.98 - ETA: 1:07 - loss: 0.0492 - accuracy: 0.98 - ETA: 1:05 - loss: 0.0504 - accuracy: 0.98 - ETA: 1:02 - loss: 0.0476 - accuracy: 0.98 - ETA: 58s - loss: 0.0448 - accuracy: 0.9859 - ETA: 53s - loss: 0.0467 - accuracy: 0.984 - ETA: 47s - loss: 0.0488 - accuracy: 0.982 - ETA: 42s - loss: 0.0469 - accuracy: 0.983 - ETA: 37s - loss: 0.0458 - accuracy: 0.984 - ETA: 32s - loss: 0.0447 - accuracy: 0.984 - ETA: 28s - loss: 0.0448 - accuracy: 0.983 - ETA: 23s - loss: 0.0458 - accuracy: 0.983 - ETA: 18s - loss: 0.0443 - accuracy: 0.983 - ETA: 13s - loss: 0.0448 - accuracy: 0.983 - ETA: 8s - loss: 0.0446 - accuracy: 0.983 - ETA: 4s - loss: 0.0463 - accuracy: 0.98 - 112s 20ms/step - loss: 0.0461 - accuracy: 0.9825 - val_loss: 0.2598 - val_accuracy: 0.9336\n",
      "Epoch 10/10\n",
      "5600/5600 [==============================] - ETA: 1:35 - loss: 0.0271 - accuracy: 0.99 - ETA: 1:32 - loss: 0.0202 - accuracy: 0.99 - ETA: 1:28 - loss: 0.0195 - accuracy: 0.99 - ETA: 1:23 - loss: 0.0211 - accuracy: 0.99 - ETA: 1:18 - loss: 0.0222 - accuracy: 0.99 - ETA: 1:13 - loss: 0.0253 - accuracy: 0.99 - ETA: 1:09 - loss: 0.0230 - accuracy: 0.99 - ETA: 1:04 - loss: 0.0215 - accuracy: 0.99 - ETA: 59s - loss: 0.0234 - accuracy: 0.9931 - ETA: 54s - loss: 0.0233 - accuracy: 0.992 - ETA: 50s - loss: 0.0242 - accuracy: 0.992 - ETA: 45s - loss: 0.0251 - accuracy: 0.991 - ETA: 40s - loss: 0.0251 - accuracy: 0.991 - ETA: 36s - loss: 0.0247 - accuracy: 0.992 - ETA: 31s - loss: 0.0247 - accuracy: 0.992 - ETA: 27s - loss: 0.0251 - accuracy: 0.992 - ETA: 22s - loss: 0.0250 - accuracy: 0.992 - ETA: 18s - loss: 0.0244 - accuracy: 0.992 - ETA: 13s - loss: 0.0243 - accuracy: 0.992 - ETA: 8s - loss: 0.0250 - accuracy: 0.992 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - 109s 19ms/step - loss: 0.0257 - accuracy: 0.9920 - val_loss: 0.2664 - val_accuracy: 0.9314\n"
     ]
    }
   ],
   "source": [
    "# Training network (model)\n",
    "hist = model.fit(matrix, label, \n",
    "           batch_size=256, epochs=10, validation_split=0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(r\"F:\\Kunal\\Job_Applications\\DeeperSystem\\train.rotfaces\")\n",
    "\n",
    "# serialising model to json\n",
    "json_model=model.to_json()\n",
    "with open('model.json','w') as file:\n",
    "    file.write(json_model)\n",
    "# serialising weights to hdf5\n",
    "model.save_weights(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and weights\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "json_file=open(\"model.json\",\"r\")\n",
    "json_model=json_file.read()\n",
    "json_file.close()\n",
    "model=model_from_json(json_model)\n",
    "model.load_weights(\"model.h5\")\n",
    "\n",
    "# after laoding model and weights, compile it and use it for precdiction and evaluation on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5361"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# getting test images\n",
    "test_imgs=os.listdir(r\"F:\\Kunal\\Job_Applications\\DeeperSystem\\test.rotfaces\\test\")\n",
    "len(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Kunal\\Job_Applications\\DeeperSystem\\test.rotfaces\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5361/5361 [01:02<00:00, 85.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5361, 64, 64, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\"F:\\Kunal\\Job_Applications\\DeeperSystem\\test.rotfaces\\test\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# reading all test images and converting them into matrix vector\n",
    "matrix_test=[]\n",
    "for img in tqdm(test_imgs):\n",
    "    im=cv2.cvtColor(cv2.imread(img,0),cv2.COLOR_GRAY2BGR)\n",
    "    matrix_test.append(np.array(im))\n",
    "matrix_test=np.array(matrix_test)\n",
    "matrix_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<built-in function imread> returned NULL without setting an error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-930365e9747d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \"\"\"\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# reading gray scale image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"image\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <built-in function imread> returned NULL without setting an error"
     ]
    }
   ],
   "source": [
    "\"\"\"# checking image\n",
    "im=cv2.imread(matrix_test[0],0) # to read image as gray image\n",
    "im=cv2.cvtColor(im,cv2.COLOR_GRAY2BGR)\n",
    "cv2.imshow(\"image\",im)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\"\n",
    "\n",
    "im=cv2.imread(matrix_test[0],0) # reading gray scale image\n",
    "cv2.imshow(\"image\",im)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5361"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# predicting the class for each image\n",
    "predictions=model.predict_classes(matrix_test)\n",
    "len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 1, 1, 1, 0, 0, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90-10184590_1979-06-16_2006.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90-1019890_1931-08-10_1978.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90-10241990_1984-11-28_2007.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90-102690_1966-09-09_2011.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90-10303590_1983-01-26_2010.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fn  prediction\n",
       "0  90-10184590_1979-06-16_2006.jpg           0\n",
       "1   90-1019890_1931-08-10_1978.jpg           2\n",
       "2  90-10241990_1984-11-28_2007.jpg           1\n",
       "3    90-102690_1966-09-09_2011.jpg           1\n",
       "4  90-10303590_1983-01-26_2010.jpg           1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a DataFrame with image_fn and predicted_class_value\n",
    "\n",
    "df=pd.DataFrame(columns=['fn','prediction'])\n",
    "df['fn']=test_imgs\n",
    "df['prediction']=list(predictions)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90-10184590_1979-06-16_2006.jpg</td>\n",
       "      <td>rotated_left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90-1019890_1931-08-10_1978.jpg</td>\n",
       "      <td>upright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90-10241990_1984-11-28_2007.jpg</td>\n",
       "      <td>rotated_right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90-102690_1966-09-09_2011.jpg</td>\n",
       "      <td>rotated_right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90-10303590_1983-01-26_2010.jpg</td>\n",
       "      <td>rotated_right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fn     prediction\n",
       "0  90-10184590_1979-06-16_2006.jpg   rotated_left\n",
       "1   90-1019890_1931-08-10_1978.jpg        upright\n",
       "2  90-10241990_1984-11-28_2007.jpg  rotated_right\n",
       "3    90-102690_1966-09-09_2011.jpg  rotated_right\n",
       "4  90-10303590_1983-01-26_2010.jpg  rotated_right"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets create one more dictionary that can be used to replace prediction values by their original orientation name\n",
    "dct={0:'rotated_left',1:'rotated_right',2:'upright',3:'upside_down'}\n",
    "for i in range(len(df)):\n",
    "    df.iloc[i,1]=dct[df.iloc[i,1]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating \"test.preds.csv\" file \n",
    "df.to_csv(r\"F:\\Kunal\\Job_Applications\\DeeperSystem\\test.rotfaces\\test.preds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting images by rotating it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5361it [00:00, 7187.42it/s]\n"
     ]
    }
   ],
   "source": [
    "#dct={'rotated_left':0,'rotated_right':1,'upright':2,'upside_down':3}\n",
    "\n",
    "\n",
    "# Rotating all predicted images to UPRIGHT (0 degree rotation) based on its rotated angle\n",
    "\n",
    "matrix_corrected=[]\n",
    "for index,pred in tqdm(enumerate(predictions)):\n",
    "    if pred==0: # for rotation_left\n",
    "        rot=cv2.getRotationMatrix2D((32,32),270,1)\n",
    "        op=cv2.warpAffine(matrix_test[index],rot,(64,64))\n",
    "        matrix_corrected.append(op)\n",
    "        \n",
    "    elif pred==1: # for rotation_right\n",
    "        rot=cv2.getRotationMatrix2D((32,32),90,1)\n",
    "        op=cv2.warpAffine(matrix_test[index],rot,(64,64))\n",
    "        matrix_corrected.append(op)\n",
    "        \n",
    "    elif pred==2: # for upright (no need to rotate image)\n",
    "        matrix_corrected.append(matrix_test[index])\n",
    "        \n",
    "    elif pred==3: # for upside_down\n",
    "        rot=cv2.getRotationMatrix2D((32,32),180,1)\n",
    "        op=cv2.warpAffine(matrix_test[index],rot,(64,64))\n",
    "        matrix_corrected.append(op)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5361, 64, 64, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting list (matrix_corrected) to numpy array\n",
    "matrix_corrected=np.array(matrix_corrected)\n",
    "matrix_corrected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all the corrected images into new folder called \"F:\\Kunal\\Job_Applications\\DeeperSystem\\test.rotfaces\\corrected\"\n",
    "\n",
    "\n",
    "os.chdir(r\"F:\\Kunal\\Job_Applications\\DeeperSystem\\test.rotfaces\\corrected\")\n",
    "\n",
    "# for corrected images, keeping same image names as test images (only changing extension to .png)\n",
    "for index,img in enumerate(test_imgs):\n",
    "    fn=\"{}.png\".format(img[:-4])\n",
    "    cv2.imwrite(fn,matrix_corrected[index])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Very first thing I tried to match the sequence of image file names from train.truth.csv and train \n",
    "2. read all the images and converted them into vector (matrix) format\n",
    "3. Verified with random images by plotting them\n",
    "4. Converted label into numerical representation after understanding about how many unique labels are present in a dataset\n",
    "5. Designed neural network architecture\n",
    "6. compiled and trained network on train dataset\n",
    "7. saved the model weights\n",
    "8. Read all test images and similar to train images , converted them into vector (matrix) format\n",
    "9. predicted the classes for each test image \n",
    "10. Rotated all those predicted images which were predicted to be as NON-UPRIGHT so that all of them can be seen UPRIGHT (i.e like non-rotated images)\n",
    "11. Finally stored the vector (matrix) representation of those rotated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Scope (How to improve model performance further)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I have sampled only 7000 images from train dataset and got decent result as we can see.\n",
    "2. So, training the same network on more images (all from train dataset) would definitely improve the accuracy.\n",
    "3. Also increasing the number of epochs would help network learn more and therefore model acuuracy will improve.\n",
    "4. Adding more convolutional layers may increase the model performance. But we have to be careful that model should not overfit.\n",
    "5. We can play around with kernel size and also number of kernels used , which may help improve model performance to some extent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions to run code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I am providing model in json format (model.json) and also weights in hdf5 (model.h5) format\n",
    "2. Run the code under \"Loading Model and Weights\" section and then you will get the model that you can use for evaluation and prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
